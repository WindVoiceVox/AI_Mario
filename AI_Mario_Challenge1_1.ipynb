{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "khB4r3Hfqplj"
      },
      "source": [
        "# 初心者をAIマリオへいざなうColab Notebook\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/WindVoiceVox/AI_Mario/blob/main/AI_Mario_Challenge1_1.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
        "</a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sfU64uUkprh2"
      },
      "source": [
        "### このColab Notebookでやること\n",
        "- ファミコン版スーパーマリオブラザーズ World 1-1を、人工知能にプレイさせてクリアをめざします。\n",
        "- この人工知能の使い方は**強化学習**と呼ばれるものです。\n",
        "- 人工知能は何度もゲームをプレイしながら、だんだん上手なプレイのしかたを学習してゆきます。このNotebookでは、約7,000回、5時間くらいのプレイでいい感じになります（運により結果はわりと変わります）。\n",
        "- ほとんど知識のない方でもNotebookのセルを順番に実行するだけでクリアできるようにしました。\n",
        "- 学習中に待っているだけだと飽きてしまうので、「変化している」ことが感じられるよう、定期的にGoogle Driveにプレイ動画を保存しています。\n",
        "- 理解のカギになる用語を太字にしてありますので、この辺りをヒントに知識を広げていくとだんだん<del>沼にはまる</del>楽しくなってくると思います。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "55vececCMSsG"
      },
      "source": [
        "### **強化学習**についてポイントだけ説明\n",
        "- **強化学習**には**「状態・行動・報酬」**の３つの要素があります……　って教科書には書いてあるのですが、そんなことを言ったとたんに「**環境**」がでてくるので大人はずるいと思ったりするかもしれません。\n",
        "- **環境**は、ゲーム機そのものです。ファミコンのエミュレータでスーパーマリオブラザーズが動いています。\n",
        "- **状態**は、ゲーム画面にいま表示されている映像です。人工知能は、画面の映像を読み取ってプレイします。\n",
        "- **行動**は、コントローラーの操作です。人工知能は**状態**をもとに「**複雑な計算**」を行い、どのようにコントローラーを操作するか、**行動**を決定します。\n",
        "- **報酬**は、**行動**の結果得られる得点のようなものです。今回はマリオが右へ進むと報酬が得られるようになっています。\n",
        "- **強化学習**は、「**状態**をもとに**行動**を決定し、その結果、**報酬**が得られたり得られなかったりする」ことを通して、人工知能が多くの報酬を得られるように**行動**を（正確には「**複雑な計算**」の中身を）少しずつ変化させてゆくものです。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yNJHNvfRT0AF"
      },
      "source": [
        "### 出典などの情報\n",
        "本当は、このような強化学習の実行にはとても複雑なプログラミングが必要ですが、インターネット上で提供されているほかの方の成果をお借りすることでとてもコンパクトにまとめることができています。\n",
        "- [PyTorch](https://pytorch.org/)\n",
        "- [OpenAI Gym](https://github.com/openai/gym)\n",
        "- [Nes-py](https://github.com/Kautenja/nes-py)\n",
        "- [gym-super-mario-bros](https://github.com/Kautenja/gym-super-mario-bros)\n",
        "- [このNotebookのもとになったNotebook](https://colab.research.google.com/github/YutaroOgawa/pytorch_tutorials_jp/blob/main/notebook/4_RL/4_2_mario_rl_tutorial_jp.ipynb#scrollTo=mMiIWSeXPKHb)\n",
        "\n",
        "また、スーパーマリオブラザーズ全部のステージクリアしようぜ！というプロジェクトも行われていて、ほかの方の成果も集まっています。\n",
        "- [からあげさんのQiitaの記事](https://qiita.com/karaage0703/items/e237887894f0f1382d58)\n",
        "- [mario-ai-challenge](https://karaage0703.github.io/mario-ai-challenge/)\n",
        "\n",
        "Google Colabが初めてという方は、インターネット上にたくさんのチュートリアルが存在するので、探してみてください。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl1jZJVdvH14"
      },
      "source": [
        "#### このNotebookに含まれている工夫\n",
        "- **強化学習**にはさまざまな手法がありますが、ここでは**DDQN**（Double Deep Q Network）を使っています。\n",
        "- **状態**を認識させるために、NNの一部としてResNet（Residual Network）を参考にしたブロックを使っています。ResNetの良い点のひとつは、入力のサイズと出力のサイズが同じなので、層を増やすことが簡単で、また層を増やすことで複雑な状態を認識できるようになることです。ただし、今回は、２ブロックしか使っていません。\n",
        "- 学習を効率よく進めるために、画面（**状態**）全体を使わずにマリオの前方だけを使っています。\n",
        "- 特に独自の工夫として、**状態**をNNに入力する前に、ランダムにブレさせる画像処理をしています。これにより**行動**の選択が少し安全側に倒されるようです。\n",
        "- **行動**も使える選択肢を限定しています。右ボタンとAボタンしか使わず、Bボタンは押したままです。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6TiFhTJ7abUL"
      },
      "source": [
        "### 用語\n",
        "軽く書いた適当な説明なので、ちゃんとした知識として知りたいかたは書籍などをあたってください。\n",
        "\n",
        "|用語|説明|備考|\n",
        "| :--- | :--- | ---- |\n",
        "|NN|人工知能の核となる、ニューラルネットワーク（Neural Network）のこと。ではニューラルネットワークとは何かというと……　Wikipediaで調べてください。| |\n",
        "|モデル|NNと、それを機能させるのに必要な付随する情報をひとまとめにしてこう呼ぶようです。 | |\n",
        "|CUDA|NVIDIA社が提供しているライブラリ。NNの処理には膨大な計算量が必要なのですが、ColabではGPUを使って高速に計算できる環境を用意しています。NVIDIAのGPUでNNの計算をするにはこの機能が必要です。 | |\n",
        "|Google Drive|みんな知っているクラウド上にファイルを保存できる場所。意外とすぐに容量が足りなくなるので注意。 | |\n",
        "|フレーム|ゲーム画面の変化をパラパラ漫画のように捉えたときの、1ページが1フレームです。 | |\n",
        "|エピソード(episode)|マリオひとり分のプレイを1エピソードと呼びます。10,000エピソード学習させるとは、つまり10,000人のマリオが犠牲になるということです。マリオが労働組合を結成しないことを祈りましょう。| |\n",
        "|ステップ(step)|状態をもとに行動をおこすことを1ステップと呼びます。つまりマリオを1回操作する機会のことです。フレームと同じじゃないの？と思われるかもしれませんが、このNotebookではいくつかのフレームをまとめて1ステップにしています。| |\n",
        "|学習(Learning)|状態をもとに行動を起こして報酬を得た結果をしばらく記憶しておいて、それをもとにNNの計算を変化させることを学習といいます。| |\n",
        "|学習率(Learning rate)|学習するとき、計算方法をちょっとだけ変化させるのですが、変化の大きさを決めるのが学習率です。数字が大きいと大きく変化させるのですが、どのくらい変化させるべきかはなかなか難しいです。| |\n",
        "|探索(Exploration)|より良い学習のためには、これまでと違う新しい行動を試してみることも必要です。ある確率に従って、ランダムにコントローラーを操作します。これを探索といいます。| |\n",
        "|探索率(Exploration rate)|探索を行う確率です。1.0にすると、いつもランダムに操作します。0.0になると、ランダムな操作は無く、いつも計算によって行動を決めます（探索の反対語として活用といいます）。探索率は教科書ではε（イプシロン）と書かれます。| |\n",
        "|環境(environment)|このプログラムではenvと書かれています。||\n",
        "|状態(state)|このプログラムではstateと書かれています。||\n",
        "|行動(action)|このプログラムではactと書かれています。||\n",
        "|報酬(reward)|このプログラムではrewardと書かれています。||\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "moduleList = sys.modules\n",
        "ENV_COLAB = False\n",
        "if 'google.colab' in moduleList:\n",
        "    print('Google Colaborateryで実行しています')\n",
        "    ENV_COLAB = True\n",
        "else:\n",
        "    print('Google Colaboratery以外の環境で実行しています')\n",
        "    ENV_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 環境を確認して、ログ保存先を設定する\n",
        "ROOT_DIR = ''\n",
        "LOG_DIR  = ''\n",
        "\n",
        "if ENV_COLAB:\n",
        "    from google.colab import drive\n",
        "    ROOT_DIR = '/content/drive'\n",
        "    LOG_DIR  = os.path.join(ROOT_DIR, 'My Drive', 'gym_mario')\n",
        "    drive.mount(ROOT_DIR)\n",
        "else:\n",
        "    ROOT_DIR = os.getcwd()\n",
        "    LOG_DIR  = os.path.join(ROOT_DIR, 'gym_mario')\n",
        "\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "print('ログの保存先:%s' % LOG_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qdPiRI_LEw0l"
      },
      "source": [
        "#### （１）パラメータ設定"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz9QK5SqaJ0p"
      },
      "source": [
        "プログラムは複雑なので、動作の微調整は、最初にここでまとめて行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IekEPVnUqXvu"
      },
      "outputs": [],
      "source": [
        "# 実行に関わるスイッチ\n",
        "DEBUG                  = True      # DEBUGをTrueにするとすべてのセルを実行。Falseにすると必須のセルだけ実行。\n",
        "LOAD_MODEL             = False     # SAVED_MODELをロードして学習を継続する場合にTrue\n",
        "DO_RECORD              = True      # 学習後にプレイ動画を保存\n",
        "USE_CUDA               = True      # CUDA使う？ Colabならもちろん使おう。\n",
        "\n",
        "# マリオが挑戦するワールドを指定\n",
        "WORLD                  = \"SuperMarioBros-1-1-v0\"\n",
        "WORLD_COLOR            = \"SuperMarioBros-1-1-v0\" # 再現録画をとるとき用\n",
        "\n",
        "# 学習済みモデルから開始する場合は*.chkptファイルをフルパス指定。\n",
        "SAVED_MODEL            = os.path.join(LOG_DIR, 'MyDrive/AI_Mario/checkpoints/yyyy-mm-ddThh-mm-ss/mario_net_x.chkpt')\n",
        "\n",
        "# 環境のパラメータ\n",
        "CROPSIZE               = 170       # もとのゲーム画面のサイズは 246x231 だが、右下の170x170の範囲を使う\n",
        "SKIPFRAMES             = 4         # 全部のフレームは計算に使わない。\n",
        "RESIZE                 = 52        # なるべく縮小して学習時間を短縮\n",
        "NUM_FRAME_STACK        = 4\n",
        "\n",
        "# 学習のパラメータ\n",
        "NUM_EPISODES           = 7000      # 学習のためマリオを走らせる人数\n",
        "BURNIN                 = 10000     # このstep数だけ記憶がたまるまでは学習を開始しない\n",
        "LEARN_EVERY            = 3         # このstep数ごとに小さな学習が行われる\n",
        "LEARNING_RATE          = 0.000300\n",
        "LEARNING_RATE_DECAY    = 0.000005\n",
        "LEARNING_RATE_MIN      = 0.000050\n",
        "SYNC_EVERY             = 10000     # このstep数ごとにonlineとtargetの2つのNNの同期が行われる\n",
        "EXPLORATION_RATE       = 1.0       # ε値の初期値(stepごとにランダムに操作する確率)\n",
        "EXPLORATION_RATE_DECAY = 0.9999975 # ε値を小さくしていく割合\n",
        "EXPLORATION_RATE_MIN   = 0.1       # ε値の最小値\n",
        "SAVE_EVERY             = 100000    # このstep数ごとにNNを保存する\n",
        "QUEUE_MAXLEN           = 58000     # GPUメモリ上にすべてのstepの(state, act)が記憶されるが、最大の保存数。\n",
        "                                   # 多くするとGPUメモリもたくさん必要になる。RESIZEが大きいと少なくする必要がある。\n",
        "                                   # GPU環境に合わせて増減させる。\n",
        "BATCH_SIZE             = 32        # 学習するときは、記憶のQUEUEからランダムにこの数だけ取り出して使う。\n",
        "GAMMA                  = 0.9       # 学習のパラメータのひとつ。まだ変更したことがない。\n",
        "MOVEMENT = [                       # マリオを操作するボタンの押し方一覧。選択肢が少ないほうが学習は短時間で進む。\n",
        "#     ['NOOP'],\n",
        "    ['B'],\n",
        "    ['right', 'B'],\n",
        "    ['right', 'A', 'B'],\n",
        "    ['A', 'B']\n",
        "]\n",
        "\n",
        "# 録画用のパラメータ\n",
        "RECORD_SUCCESS_XPOS = 3200         # 録画時、このXを超えるか、ゴールできたら「成功」として録画する。World 1-1は、X=3200くらいでゴール\n",
        "RECORD_NFILES       = 10           # この数だけ成功録画を録画する。\n",
        "RECORD_EPSILON      = 0.1          # 録画の時に使用するε値"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G01DDuJEKERf"
      },
      "source": [
        "##### （２）実行環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkd4mMOAUbt1"
      },
      "outputs": [],
      "source": [
        "# Colab環境にマリオをインストール\n",
        "!pip install gym-super-mario-bros==7.3.2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5olPXBWDWM86"
      },
      "source": [
        "#### （３）モジュールのロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHWlwFKOPKHI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import pickle\n",
        "from IPython import display\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchsummary import summary\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IzaXrzDM20ew"
      },
      "source": [
        "#### （４）環境(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RruZEHkgPKHM"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"スキップした後のフレームのみを返す\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"行動を繰り返し、報酬を合計する\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # 報酬を蓄積し、同じ行動を繰り返す\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # [H, W, C] のarrayを、[C, H, W] のtensorに変換\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "class MyShiftTransform:\n",
        "    \"\"\"\n",
        "    画面をランダムに少しズラすことで、視界を乱して、マリオが少し余裕をみて操作されるように学習する。\n",
        "    ResizeObservationの中で使用される。\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        nshift = random.random() * 5\n",
        "        return TF.affine(x, angle=0, translate=(nshift, 0), scale=1.0, shear=0)\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose([  \n",
        "            T.Resize(self.shape),\n",
        "            MyShiftTransform(), # カスタマイズ\n",
        "            T.Normalize(0, 255)\n",
        "        ])\n",
        "        obs = T.FiveCrop(size=(CROPSIZE, CROPSIZE))(observation)[3] # 画面の右下だけ切り抜き\n",
        "        observation = transforms(obs).squeeze(0)\n",
        "        return observation\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TqQfXjlcTI"
      },
      "source": [
        "#### （５）【参考】環境(env)の確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2ASJOjd1C39"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    # 画面をNNの入力として使えるように加工する様子を確認できます。\n",
        "    env5 = gym_super_mario_bros.make(WORLD)\n",
        "    env4 = JoypadSpace(env5, MOVEMENT)\n",
        "    env3 = SkipFrame(env4, skip=SKIPFRAMES)\n",
        "    env2 = GrayScaleObservation(env3)\n",
        "    env1 = ResizeObservation(env2, shape=RESIZE)\n",
        "    env  = FrameStack(env1, num_stack=NUM_FRAME_STACK)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oonoLqnK2FTv"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    env.reset()\n",
        "    ret5 = env5.step(action=0)\n",
        "    ret4 = env4.step(action=0)\n",
        "    ret3 = env3.step(action=0)\n",
        "    ret2 = env2.step(action=0)\n",
        "    ret1 = env1.step(action=0)\n",
        "    ret  = env.step(action=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtJC96Ex5nlO"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    # もとのゲーム画面\n",
        "    print(ret5[0].shape)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(ret5[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX65axYS50R6"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    # Joypadで操作できるようにする（見た目は変化なし）\n",
        "    print(ret4[0].shape)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(ret4[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWxcEv7E53c6"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    # フレームをスキップするようにする（報酬は足し合わされている）\n",
        "    print(ret3[0].shape)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(ret3[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KoVXUqb8L3U"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    # データサイズを減らすためグレースケールにする\n",
        "    print(ret2[0].shape)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(ret2[0][0,:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyvk4i7x8nTD"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    # サイズを落としている\n",
        "    print(ret1[0].shape)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(ret1[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfI9udEP9DEz"
      },
      "outputs": [],
      "source": [
        "def display_all_frame():\n",
        "    plt.figure(figsize=(16,16))\n",
        "    for idx in range(ret[0].shape[0]):\n",
        "        plt.subplot(1, NUM_FRAME_STACK, idx+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(ret[0][idx])\n",
        "    plt.show()\n",
        "\n",
        "if DEBUG:\n",
        "    # 画面の変化がわかるよう、NUM_FRAME_STACK枚だけまとめて入力する\n",
        "    print(ret[0].shape)\n",
        "    display_all_frame()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "amalK4PLakUx"
      },
      "source": [
        "#### （６）環境(env)の作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvU5F3jJlECw"
      },
      "outputs": [],
      "source": [
        "# 環境の作成\n",
        "env = gym_super_mario_bros.make(WORLD)\n",
        "env = JoypadSpace(env, MOVEMENT)\n",
        "env = SkipFrame(env, skip=SKIPFRAMES)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=RESIZE)\n",
        "env = FrameStack(env, num_stack=NUM_FRAME_STACK)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MGxn8os-IuoE"
      },
      "source": [
        "#### （７）ニューラルネットワークの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCB2xir4PKHV"
      },
      "outputs": [],
      "source": [
        "class MarioBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv2d1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn1     = nn.BatchNorm2d(16)\n",
        "        self.conv2d2 = nn.Conv2d(in_channels=16, out_channels=4, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn2     = nn.BatchNorm2d(in_channels)\n",
        "        self.relu    = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        output = self.conv2d1(x)\n",
        "        output = self.bn1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.conv2d2(output)\n",
        "        output = self.bn2(output)\n",
        "        output += x\n",
        "        output = self.relu(output)\n",
        "        return output\n",
        "\n",
        "class MarioNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            MarioBlock(c),             # 主に画面を認識する部分\n",
        "            MarioBlock(c),             # 主に画面を認識する部分\n",
        "            MarioBlock(c),             # 主に画面を認識する部分\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c*h*w, 96),     # 操作を選択する部分\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, output_dim) # 操作を選択する部分\n",
        "        )\n",
        "\n",
        "        # DQNでは、targetネットワークは普段は変更せず、定期的にonlineと同期される。\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # targetはパラメータが変更されないよう固定する。\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc1KmaDSEdGf"
      },
      "source": [
        "#### （８）エージェントの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tjQ56JIPKHP"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim              = state_dim\n",
        "        self.action_dim             = action_dim\n",
        "        self.save_dir               = save_dir\n",
        "        self.use_cuda               = torch.cuda.is_available()\n",
        "        if USE_CUDA == False:\n",
        "            self.use_cuda = False\n",
        "        \n",
        "        # torch.compileはまだWindowsでは実行できないとのこと(2023.05.06時点)\n",
        "        # RuntimeError: Windows not yet supported for torch.compile\n",
        "        #nn                          = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        #self.net                    = torch.compile(nn)\n",
        "        self.net                    = MarioNet(self.state_dim, self.action_dim).float()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.net                = self.net.to(device=\"cuda\")\n",
        "        self.exploration_rate       = EXPLORATION_RATE\n",
        "        self.exploration_rate_decay = EXPLORATION_RATE_DECAY\n",
        "        self.exploration_rate_min   = EXPLORATION_RATE_MIN\n",
        "        self.total_episodes         = 0\n",
        "        self.total_steps            = 0\n",
        "        self.curr_step              = 0\n",
        "        self.save_every             = SAVE_EVERY\n",
        "        self.memory                 = deque(maxlen=QUEUE_MAXLEN)\n",
        "        self.pool                   = []\n",
        "        self.batch_size             = BATCH_SIZE\n",
        "        self.gamma                  = GAMMA\n",
        "        self.optimizer              = torch.optim.Adam(self.net.parameters(), lr=LEARNING_RATE)\n",
        "        self.loss_fn                = torch.nn.SmoothL1Loss()\n",
        "        self.burnin                 = BURNIN       # 経験を訓練させるために最低限必要なステップ数\n",
        "        self.learn_every            = LEARN_EVERY  # Q_onlineを更新するタイミングを示すステップ数\n",
        "        self.sync_every             = SYNC_EVERY   # Q_target & Q_onlineを同期させるタイミングを示すステップ数\n",
        "\n",
        "    def act(self, state, epsilon=-1):\n",
        "        \"\"\"\n",
        "        MarioNetに3個の情報を入力し、Marioの操作(action)を決定する。\n",
        "        state   ... 画面の状態\n",
        "        epsilon ... 通常はself.exploration_rateでε値が決まるが、引数で変更することも可能。\n",
        "        \"\"\"\n",
        "        if ((epsilon == -1) and (np.random.rand() < self.exploration_rate)) or (np.random.rand() < epsilon):\n",
        "            # 【探索】ランダム値がε値以下の場合、適当な操作をする\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "        else:\n",
        "            # 【活用】MarioNetを使用して、もっとも成功が見込める操作をする\n",
        "            state = state.__array__()\n",
        "            if self.use_cuda:\n",
        "                state = torch.tensor(state).cuda()\n",
        "            else:\n",
        "                state = torch.tensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # exploration_rateを減衰させる\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # ステップを+1します\n",
        "        self.curr_step += 1\n",
        "        self.total_steps += 1\n",
        "        return action_idx\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        経験をself.memory (replay buffer)に保存します\n",
        "\n",
        "        Inputs:\n",
        "            state (LazyFrame),\n",
        "            next_state (LazyFrame),\n",
        "            action (int),\n",
        "            reward (float),\n",
        "            done(bool))\n",
        "        \"\"\"\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            state = torch.tensor(state).cuda()\n",
        "            next_state = torch.tensor(next_state).cuda()\n",
        "            action = torch.tensor([action]).cuda()\n",
        "            reward = torch.tensor([reward]).cuda()\n",
        "            done = torch.tensor([done]).cuda() # ToDo: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
        "            #done = torch.tensor([int(done)]).cuda() # ToDo: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
        "        else:\n",
        "            state = torch.tensor(state)\n",
        "            next_state = torch.tensor(next_state)\n",
        "            action = torch.tensor([action])\n",
        "            reward = torch.tensor([reward])\n",
        "            done = torch.tensor([done])\n",
        "            #done = torch.tensor([int(done)])\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "    def pool_cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        経験を学習するか一時的に保留しておきます。\n",
        "        \"\"\"\n",
        "        self.pool.append((state, next_state, action, reward, done,))\n",
        "\n",
        "    def delete_pooled_cache(self):\n",
        "        \"\"\"\n",
        "        pool_cacheで保留されていた経験を破棄します。\n",
        "        \"\"\"\n",
        "        self.pool.clear()\n",
        "        \n",
        "    def remember_pooled_cache(self, times=1):\n",
        "        \"\"\"\n",
        "        pool_cacheで保留されていた経験を記憶します。\n",
        "        \"\"\"\n",
        "        for i in range(times):\n",
        "            for m in self.pool:\n",
        "                self.cache(m[0], m[1], m[2], m[3], m[4])\n",
        "        self.pool.clear()\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        学習用に、記憶からまとまった量をランダムに取り出します\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "        td_est = self.td_estimate(state, action)\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate, total_episodes=self.total_episodes, total_steps=self.total_steps),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pMOm6tWFPKHZ"
      },
      "source": [
        "#### （９）ログの保存方法を定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F18Bh7QPKHZ"
      },
      "outputs": [],
      "source": [
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        self.save_logX = save_dir / \"logX\"\n",
        "\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot    = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot    = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot     = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # 指標の履歴\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "        self.ep_posXs = []\n",
        "\n",
        "        # record()が呼び出されるたびに追加される移動平均\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # 現在のエピソードの指標\n",
        "        self.init_episode()\n",
        "\n",
        "        # 時間を記録\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q, posX):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        self.curr_ep_posX    = posX\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"エピソード終了時の記録\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        self.ep_posXs.append(str(self.curr_ep_posX))\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "        self.curr_ep_posX = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print('Episode %6d - Step %8d - Epsilon %1.9f - Reward %3.3f - Length %7s - Loss %1.3f - Q Value %3.3f - Time Delta %8s - Time %s' % \n",
        "              (episode, step, epsilon, mean_ep_reward, str('%3.3f' %(mean_ep_length)),\n",
        "               mean_ep_loss, mean_ep_q, str('%3.3f' %(time_since_last_record)), \n",
        "               datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y-%m-%dT%H:%M:%S')))\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:9d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        with open(self.save_logX, \"a\") as f:\n",
        "            oneline = \",\".join(self.ep_posXs)\n",
        "            f.write(\n",
        "                f\"{oneline}\\n\"\n",
        "            )\n",
        "            self.ep_posXs = []\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.title(metric, loc='right')\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ZLglLePKHa"
      },
      "source": [
        "#### （１０）実行の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MccVRzwimic",
        "outputId": "a61119bc-f2fb-4a10-b03d-6882d5b7e257"
      },
      "outputs": [],
      "source": [
        "# ログの保管場所を作成\n",
        "dt_now_JST = datetime.timezone(datetime.timedelta(hours=9))\n",
        "save_dir = Path(LOG_DIR + \"/checkpoints\") / datetime.datetime.now(dt_now_JST).strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "save_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYP1qvnhgYd7"
      },
      "outputs": [],
      "source": [
        "# Marioの作成\n",
        "mario = Mario(state_dim=(NUM_FRAME_STACK, RESIZE, RESIZE), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "# その他のログ出力を準備\n",
        "logger = MetricLogger(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luoIjaU-GBHO"
      },
      "outputs": [],
      "source": [
        "# マリオの中身をざっくり確認\n",
        "summary(mario.net.online, (SKIPFRAMES,RESIZE,RESIZE))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RO3TvH56wqaj"
      },
      "source": [
        "#### （１１）【任意】学習結果の復元\n",
        "\n",
        "過去の状態を復元したいときは、上のMarioの作成を実行したあと、オブジェクトのデータを上書きする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcgYRBQiewbu"
      },
      "outputs": [],
      "source": [
        "if LOAD_MODEL and SAVED_MODEL != '':\n",
        "    # checkpointからデータを読み込み\n",
        "    print('Model Loading... %s' % SAVED_MODEL)\n",
        "    loaded_data = torch.load(SAVED_MODEL)\n",
        "    # marioオブジェクトに値をロード\n",
        "    mario.net.load_state_dict(loaded_data['model'])\n",
        "    mario.exploration_rate = loaded_data['exploration_rate']\n",
        "    mario.total_episodes   = loaded_data['total_episodes']\n",
        "    mario.total_steps      = loaded_data['total_steps']\n",
        "    print('exploration_rate:%f' % (mario.exploration_rate))\n",
        "    print('total_episodes  :%d' % (mario.total_episodes))\n",
        "    print('total_steps     :%f' % (mario.total_steps))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr58ij3e1uR8"
      },
      "source": [
        "#### （１２）動画保存の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03BshS7QWM9D"
      },
      "outputs": [],
      "source": [
        "def make_play_movie(img_reserve, number, title=''):\n",
        "    # カラーでの動画を表示\n",
        "    matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
        "\n",
        "    dpi = 72\n",
        "    interval = 17 # ms\n",
        "\n",
        "    plt.figure(figsize=(240/dpi, 256/dpi), dpi=dpi)\n",
        "    # 軸は消す\n",
        "    plt.axis('off')\n",
        "    plt.title(title, loc='right')\n",
        "    patch = plt.imshow(img_reserve[0]) # patch: AxesImage class\n",
        "    animate = lambda i: patch.set_data(img_reserve[i])\n",
        "    ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(img_reserve), interval=interval)\n",
        "    ani.save(os.path.join(save_dir, ('ai_mario%05d.mp4' % (number))))\n",
        "    # display.display(display.HTML(ani.to_jshtml()))\n",
        "    plt.close()\n",
        "\n",
        "def make_env_for_movie():\n",
        "    env_color = gym_super_mario_bros.make(WORLD_COLOR)\n",
        "    return JoypadSpace(env_color, MOVEMENT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tIeNDkI7tvAs"
      },
      "source": [
        "#### （１３）学習の実施"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeOs-Jw6PKHa"
      },
      "outputs": [],
      "source": [
        "episodes = NUM_EPISODES       # 学習回数\n",
        "maxX = 1                      # 最大到達点を更新\n",
        "goals = 0                     # ゴールできた回数を記録\n",
        "history = 0\n",
        "reward_history = [0] * 100    # 報酬の履歴を記録\n",
        "estart = mario.total_episodes # 学習を再開するときのための処理\n",
        "env_color = make_env_for_movie()\n",
        "\n",
        "for e in range(estart, episodes):\n",
        "\n",
        "    # Learning Rateを定期的に減らす\n",
        "    if e % 1000 == 0 and LEARNING_RATE > LEARNING_RATE_MIN:\n",
        "        LEARNING_RATE -= LEARNING_RATE_DECAY\n",
        "        # 下限を突き抜けないようにする\n",
        "        if LEARNING_RATE < LEARNING_RATE_MIN:\n",
        "            LEARNING_RATE = LEARNING_RATE_MIN\n",
        "        mario.optimizer = torch.optim.Adam(mario.net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 100回に1回は録画をする\n",
        "    if e % 100 == 0:\n",
        "        rec = True\n",
        "        img_color = []\n",
        "        env_color.reset() \n",
        "    else:\n",
        "        rec = False\n",
        "\n",
        "    # プレイ開始前に環境をリセット\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    # 1回のプレイを開始！\n",
        "    while True:\n",
        "\n",
        "        # 【行動】ε値は最初大きすぎてランダムに操作しすぎるので、10エピソードを1単位として、変化させている。\n",
        "        ep = (e % 10) * 0.1\n",
        "        if ep > mario.exploration_rate:\n",
        "            action = mario.act(state)             # NNに設定されたε値で行動\n",
        "        else:\n",
        "            action = mario.act(state, epsilon=ep) # (小さく)カスタマイズされたε値で行動\n",
        "\n",
        "        # 【環境】操作に応じて環境は変化する。\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # 録画用（4 skipしているので4回同じ行動をします）\n",
        "        if rec:\n",
        "            for i in range(SKIPFRAMES):\n",
        "                next_state_color, _, done2, _ = env_color.step(action)\n",
        "                rgb_img_color = np.stack(next_state_color, axis=0)\n",
        "                img_color.append(rgb_img_color)\n",
        "                if done2:\n",
        "                    break\n",
        "\n",
        "        # 【記憶】すぐにはcacheせずに、いったんpoolする。1エピソードが終わってから記憶するか決める。\n",
        "        mario.pool_cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # 【学習】learn内部では記憶に応じて学習している\n",
        "        q, loss = mario.learn()\n",
        "        logger.log_step(reward, loss, q, info[\"x_pos\"])\n",
        "\n",
        "        # 状態の更新\n",
        "        state = next_state\n",
        "\n",
        "        # 1エピソードが終了したか確認\n",
        "        if done or info[\"flag_get\"]:\n",
        "            # 終了時、到達点を記録\n",
        "            reward_history[history] = info[\"x_pos\"]\n",
        "            history = (history + 1) % 100\n",
        "            # 最長不倒記録は更新した？\n",
        "            if info[\"x_pos\"] > maxX:\n",
        "                maxX = info[\"x_pos\"]\n",
        "                print(f\"<<< 最長不倒更新! %d >>>\" % info[\"x_pos\"])\n",
        "            if info[\"flag_get\"]:\n",
        "                # ゴールできたら知りたいですよね\n",
        "                goals += 1\n",
        "                print(f\"<<< ゴール! %d回目 >>>\" % goals)\n",
        "            \n",
        "            # 今回のエピソードを記憶する\n",
        "            mario.remember_pooled_cache()\n",
        "\n",
        "            # 録画用の処理\n",
        "            if rec:\n",
        "                title = 'Episode:%4d' % (e)\n",
        "                make_play_movie(img_color, mario.total_episodes, title=title)\n",
        "\n",
        "            # 1回のプレイを終了\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "    if e % 20 == 0 and e != 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
        "    mario.total_episodes += 1\n",
        "\n",
        "# 予定のエピソードを消化したら終わり。最後に保存。お疲れさまでした。\n",
        "mario.save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fMBvVzbsuZS9"
      },
      "source": [
        "#### （１４）AIプレイ動画の作成"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "95cloKjbWM9D"
      },
      "source": [
        "学習が終わったら、最後に好プレイ動画を予定数保存します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1bOWpF8uyTs"
      },
      "outputs": [],
      "source": [
        "if DO_RECORD:\n",
        "    # 学習環境は、データが削減されておりプレイ動画の作成には向いていません。\n",
        "    # 別の環境を用意し、同じactionを両方の環境に適用することで、通常の画面のほうで動画を撮ります。\n",
        "    env_color = gym_super_mario_bros.make(WORLD_COLOR)\n",
        "    env_color = JoypadSpace(env_color, MOVEMENT)\n",
        "\n",
        "    max_reward=0\n",
        "    img_reserve=[]\n",
        "    make_n_moviefiles = 0\n",
        "\n",
        "    while make_n_moviefiles < RECORD_NFILES:\n",
        "        # 環境と状態を初期化します\n",
        "        state_color = env_color.reset()\n",
        "        state = env.reset()\n",
        "        # action_replay = [] # actionの履歴を全部保存する\n",
        "        \n",
        "        # 動画作成用の画像を溜めるリスト\n",
        "        img = []\n",
        "        img_color=[]\n",
        "\n",
        "        # うまくいった？\n",
        "        total_reward = 0\n",
        "        # step数\n",
        "        num_step = 0\n",
        "\n",
        "        # ゲーム開始！\n",
        "        while True:\n",
        "\n",
        "            # 現在の状態に対するエージェントの行動を決める\n",
        "            action = mario.act(state, epsilon=RECORD_EPSILON)\n",
        "            # action_replay.append(action)\n",
        "\n",
        "            # エージェントが行動を実行\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            # 「本番」は学習せず、記録にも残さない。\n",
        "            # grayscaleの画像をRGBの画像に変換\n",
        "            rgb_img = np.stack((state[0],)*3,axis=0).transpose(1,2,0)\n",
        "            img.append(rgb_img)\n",
        "            # 録画用（4 skipしているので4回同じ行動をします）\n",
        "            for i in range(SKIPFRAMES):\n",
        "                next_state_color, _, done2, _ = env_color.step(action)\n",
        "                rgb_img_color = np.stack(next_state_color, axis=0)\n",
        "                img_color.append(rgb_img_color)\n",
        "                if done2:\n",
        "                    break\n",
        "\n",
        "            # 状態の更新\n",
        "            state = next_state\n",
        "            num_step+=1\n",
        "            for f in range(state.shape[0]):\n",
        "                state[f][0][0] = info['x_pos'] / 10000\n",
        "\n",
        "            # ゲームが終了したかどうかを確認\n",
        "            if done or done2 or info[\"flag_get\"]:\n",
        "                if info[\"flag_get\"]:\n",
        "                    print(f\"<<< ゴール! %d >>>\" % info[\"x_pos\"])\n",
        "                break\n",
        "\n",
        "        print(\"num_step: %4d, reward: %5d xpos: %4d max: %4d\" % ( num_step, total_reward, info['x_pos'], max_reward))\n",
        "\n",
        "        if max_reward < total_reward:\n",
        "            max_reward = total_reward\n",
        "            img_reserve = copy.copy(img_color)\n",
        "\n",
        "        # 既定の回数成功するまで繰り返し\n",
        "        if info['x_pos'] > RECORD_SUCCESS_XPOS or info[\"flag_get\"]:\n",
        "            print(\"GG! Recording...\")\n",
        "            make_play_movie(img_color, make_n_moviefiles, title='Learning Complete')\n",
        "            make_n_moviefiles += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cProfile, pstats, io\n",
        "from pstats import SortKey\n",
        "\n",
        "pr = cProfile.Profile()\n",
        "pr.enable()\n",
        "\n",
        "n1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pr.disable()\n",
        "pr.print_stats(sort='time')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B6TqQfXjlcTI"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "b14c456527af9c08ddaff65f16f6e0b634ea603c1cea965d09d159818fd2eed0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
